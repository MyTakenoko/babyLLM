# babyLLM

因为没有A100，也没有5090，所以尝试白嫖google colab来训练一个小模型。

---

## 项目：练手级 Baby-Llama 训练

随便记录一下，在 Colab 的 T4 上跑的一个超小规模模型训练过程。

### 核心配置

为了不让显存炸掉，模型压得很小：

| 参数 | 配置 |
| --- | --- |
| **模型架构** | Llama (134.11M params) |
| **层数/头数** | 12 Layers / 12 Heads |
| **隐层维度** | 768 |
| **数据集** | 悟道 (WuDao) 中文语料 |
| **上下文长度** | 512 |

### 训练要点

* **流式加载**：数据集太大，没空等下载，直接用 `streaming=True` 边下边练。
* **混合精度**：开了 `fp16`，不然 T4 跑不动。
* **步数**：跑了 5000 步，Loss 从 8.7 降到了 2.1 左右。

### 随手测试

模型保存到了 Google Drive。输入“北京是中国的”，它吐出来的结果如下：

> “北京是中国的国际交流合作伙伴,是由于它颇有创意,这个伙伴的普及。估计我们也知道大爷大儿子这是为什么？...”

语义虽然有点抽象，但基本的人话逻辑还在。
